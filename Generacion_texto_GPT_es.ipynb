{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efa9131d",
   "metadata": {},
   "source": [
    "# Práctica 5 · Generación de Texto con GPT (Español)\n",
    "\n",
    "**Autores:** Javier Ricardo Muñoz Castiollo · Yazmin Johana Garcia\n",
    "**Trabajo:** Generador de leyendas ficticias con GPT-2 en español\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ohtar10/icesi-nlp/blob/main/Sesion5/1-text-generation.ipynb)\n",
    "\n",
    "En este notebook usaremos un modelo tipo GPT‑2 preentrenado en español para **generar texto** a partir de un *prompt* inicial. Luego haremos **ajuste fino (fine‑tuning)** con un pequeño corpus para observar cómo cambia el estilo de la generación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dbf04a",
   "metadata": {},
   "source": [
    "## Resumen de la actividad\n",
    "\n",
    "- Se configuró un entorno de generación con `DeepESP/gpt2-spanish`, asegurando compatibilidad con GPU CUDA y MPS.\n",
    "- Se preparó un corpus propio de leyendas ficticias en formato JSONL y se realizó un EDA breve.\n",
    "- Se tokenizó y dividió el conjunto en entrenamiento/validación para fine-tuning causal.\n",
    "- Se afinó el modelo con `Trainer` y se generaron ejemplos usando prompts temáticos de leyendas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c1f18",
   "metadata": {},
   "source": [
    "\n",
    "#### Referencias\n",
    "- Dataset de chistes: https://huggingface.co/datasets/mrm8488/CHISTES_spanish_jokes\n",
    "- Radford et al. (2018): [Improving Language Understanding by Generative Pre‑Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "- Libro: *Natural Language Processing with Transformers* (O'Reilly) — Cap. 5\n",
    "- Modelo: [GPT‑2 Spanish (DeepESP)](https://huggingface.co/DeepESP/gpt2-spanish)\n",
    "- Guía: [Fine‑tune a non‑English GPT‑2 model with Hugging Face](https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b536fabe",
   "metadata": {},
   "source": [
    "\n",
    "## GPT  (diferencias con BERT)\n",
    "\n",
    "- **Arquitectura**: GPT apila bloques de **Transformer Decoder**; BERT utiliza **Transformer Encoder**.\n",
    "- **Objetivo de preentrenamiento**: GPT predice el **siguiente token** (*causal LM*); BERT enmascara tokens (**MLM**) para construir representaciones bidireccionales.\n",
    "- **Uso típico**: GPT se emplea para **generación de texto**; BERT para **comprensión/representación**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b247ff0",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Preparación del entorno\n",
    "Ejecuta la siguiente celda para detectar si estás en Colab y, de ser así, instalar dependencias opcionales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9574fd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_COLAB: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pkg_resources, warnings, sys, subprocess, os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "installed = [p.key for p in pkg_resources.working_set]\n",
    "IN_COLAB = 'google-colab' in installed\n",
    "\n",
    "def pip_install(requirements):\n",
    "    print(\"Instalando dependencias...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", requirements])\n",
    "\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        import urllib.request\n",
    "        url = \"https://raw.githubusercontent.com/Ohtar10/icesi-nlp/main/requirements.txt\"\n",
    "        local = \"requirements.txt\"\n",
    "        urllib.request.urlretrieve(url, local)\n",
    "        pip_install(local)\n",
    "    except Exception as e:\n",
    "        print(\"No se pudieron instalar dependencias desde el repositorio:\", e)\n",
    "\n",
    "print(\"IN_COLAB:\", IN_COLAB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86f8d10",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Cargar el modelo y el tokenizador\n",
    "Usaremos `DeepESP/gpt2-spanish` y movemos el modelo a GPU si está disponible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6564364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"DeepESP/gpt2-spanish\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32045e0d",
   "metadata": {},
   "source": [
    "\n",
    "### Inspección rápida de módulos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27dd9123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'transformer',\n",
       " 'transformer.wte',\n",
       " 'transformer.wpe',\n",
       " 'transformer.drop',\n",
       " 'transformer.h',\n",
       " 'transformer.h.0',\n",
       " 'transformer.h.0.ln_1',\n",
       " 'transformer.h.0.attn',\n",
       " 'transformer.h.0.attn.c_attn',\n",
       " 'transformer.h.0.attn.c_proj',\n",
       " 'transformer.h.0.attn.attn_dropout',\n",
       " 'transformer.h.0.attn.resid_dropout',\n",
       " 'transformer.h.0.ln_2',\n",
       " 'transformer.h.0.mlp',\n",
       " 'transformer.h.0.mlp.c_fc',\n",
       " 'transformer.h.0.mlp.c_proj',\n",
       " 'transformer.h.0.mlp.act',\n",
       " 'transformer.h.0.mlp.dropout',\n",
       " 'transformer.h.1',\n",
       " 'transformer.h.1.ln_1',\n",
       " 'transformer.h.1.attn',\n",
       " 'transformer.h.1.attn.c_attn',\n",
       " 'transformer.h.1.attn.c_proj',\n",
       " 'transformer.h.1.attn.attn_dropout']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "modules = [m for m, _ in model.named_modules()]\n",
    "modules[:25]  # muestra los primeros 25 para no saturar la salida\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9618574",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Demostración: *forward pass* y logits\n",
    "Observamos dimensiones de entrada/salida, extraemos el último vector de logits y calculamos probabilidades de los **top‑k** siguientes tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5b57fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de la entrada: torch.Size([1, 3])\n",
      "Dimensiones de la salida: torch.Size([1, 3, 50257])\n",
      "Dimensiones del último token de la secuencia: torch.Size([50257])\n",
      "Dimensiones de las probabilidades: torch.Size([50257])\n",
      "{' más': '40.87%', ' que': '10.55%', ' en': '7.27%', ',': '4.90%', ' allí': '0.99%', ' se': '0.87%', '.': '0.82%', ' dentro': '0.81%', ' a': '0.78%', ' al': '0.78%'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "prompt = \"Había una vez\"\n",
    "best = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokens = tokenizer(prompt, return_tensors='pt')['input_ids'].to(device)\n",
    "    print(\"Dimensiones de la entrada:\", tokens.shape)\n",
    "    output = model(input_ids=tokens)\n",
    "    print(\"Dimensiones de la salida:\", output.logits.shape)\n",
    "    last_logits = output.logits[0, -1, :]\n",
    "    print(\"Dimensiones del último token de la secuencia:\", last_logits.shape)\n",
    "    probs = torch.softmax(last_logits, dim=-1)\n",
    "    print(\"Dimensiones de las probabilidades:\", probs.shape)\n",
    "    topk = torch.topk(probs, best)\n",
    "    print({tokenizer.decode(i): f\"{p.item()*100:.2f}%\" for i, p in zip(topk.indices, topk.values)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fac5d6",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Función de generación personalizada (*ε-greedy* + muestreo)\n",
    "\n",
    "- `eps=1.0` → **greedy** puro (siempre el token más probable).\n",
    "- `eps=0.0` → **muestreo** puro (siempre samplea de la distribución).\n",
    "- `0 < eps < 1` → mezcla: con prob. `eps` usa greedy; en otro caso, muestrea.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f24d4ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Tuple\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "\n",
    "def generate(\n",
    "        model: nn.Module,\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "        start: str,\n",
    "        max_length: int = 100,\n",
    "        eps: float = 0.5,\n",
    "        top_n: int = 5,\n",
    "        return_iterations: bool = False,\n",
    "        device: Optional[torch.device] = None\n",
    "    ) -> Tuple[str, Optional[pd.DataFrame]]:\n",
    "\n",
    "    output = [start]\n",
    "    iterations = []\n",
    "    target_device = device if device is not None else next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(output[-1], return_tensors='pt').to(target_device)\n",
    "        input_ids = inputs['input_ids']\n",
    "        for _ in range(max_length):\n",
    "            logits = model(input_ids=input_ids).logits\n",
    "            probs = torch.softmax(logits[0, -1, :], dim=-1)\n",
    "            sorted_tokens = torch.argsort(probs, dim=-1, descending=True)\n",
    "\n",
    "            if np.random.random_sample(1)[0] < eps:\n",
    "                next_token = sorted_tokens[0].unsqueeze(dim=0)\n",
    "            else:\n",
    "                next_token = torch.multinomial(probs, 1)\n",
    "\n",
    "            if return_iterations:\n",
    "                step = {'input': ''.join(output)}\n",
    "                best_n = sorted_tokens[:top_n].cpu().tolist()\n",
    "                choices = {f'Choice #{i+1}': f\"{tokenizer.decode([tok], skip_special_tokens=True)} ({probs[best_n[i]].item():.4f})\"\n",
    "                           for i, tok in enumerate(best_n)}\n",
    "                step.update(choices)\n",
    "                iterations.append(step)\n",
    "\n",
    "            output.append(tokenizer.decode(next_token, skip_special_tokens=True))\n",
    "            next_token = next_token.unsqueeze(dim=0)\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "        text_out = ''.join(output)\n",
    "        if not return_iterations:\n",
    "            return text_out, None\n",
    "        df = pd.DataFrame(iterations)\n",
    "        return text_out, df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478d0cfe",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1 Greedy (ε = 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0d4f666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Había una vez más, el hombre que había sido su padre, el que había sido su\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>Choice #1</th>\n",
       "      <th>Choice #2</th>\n",
       "      <th>Choice #3</th>\n",
       "      <th>Choice #4</th>\n",
       "      <th>Choice #5</th>\n",
       "      <th>Choice #6</th>\n",
       "      <th>Choice #7</th>\n",
       "      <th>Choice #8</th>\n",
       "      <th>Choice #9</th>\n",
       "      <th>Choice #10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Había una vez</td>\n",
       "      <td>más (0.4087)</td>\n",
       "      <td>que (0.1055)</td>\n",
       "      <td>en (0.0727)</td>\n",
       "      <td>, (0.0490)</td>\n",
       "      <td>allí (0.0099)</td>\n",
       "      <td>se (0.0087)</td>\n",
       "      <td>. (0.0082)</td>\n",
       "      <td>dentro (0.0081)</td>\n",
       "      <td>a (0.0078)</td>\n",
       "      <td>al (0.0078)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Había una vez más</td>\n",
       "      <td>, (0.5323)</td>\n",
       "      <td>en (0.0701)</td>\n",
       "      <td>. (0.0519)</td>\n",
       "      <td>la (0.0194)</td>\n",
       "      <td>que (0.0191)</td>\n",
       "      <td>: (0.0174)</td>\n",
       "      <td>el (0.0162)</td>\n",
       "      <td>se (0.0157)</td>\n",
       "      <td>a (0.0142)</td>\n",
       "      <td>y (0.0113)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Había una vez más,</td>\n",
       "      <td>el (0.0491)</td>\n",
       "      <td>se (0.0456)</td>\n",
       "      <td>la (0.0409)</td>\n",
       "      <td>en (0.0339)</td>\n",
       "      <td>y (0.0301)</td>\n",
       "      <td>no (0.0299)</td>\n",
       "      <td>había (0.0205)</td>\n",
       "      <td>me (0.0194)</td>\n",
       "      <td>su (0.0158)</td>\n",
       "      <td>los (0.0157)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Había una vez más, el</td>\n",
       "      <td>hombre (0.0230)</td>\n",
       "      <td>hecho (0.0225)</td>\n",
       "      <td>señor (0.0173)</td>\n",
       "      <td>joven (0.0165)</td>\n",
       "      <td>recuerdo (0.0157)</td>\n",
       "      <td>muchacho (0.0120)</td>\n",
       "      <td>mundo (0.0119)</td>\n",
       "      <td>corazón (0.0105)</td>\n",
       "      <td>cuerpo (0.0095)</td>\n",
       "      <td>deseo (0.0087)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Había una vez más, el hombre</td>\n",
       "      <td>que (0.1364)</td>\n",
       "      <td>se (0.0979)</td>\n",
       "      <td>de (0.0590)</td>\n",
       "      <td>había (0.0475)</td>\n",
       "      <td>no (0.0358)</td>\n",
       "      <td>le (0.0276)</td>\n",
       "      <td>estaba (0.0216)</td>\n",
       "      <td>la (0.0205)</td>\n",
       "      <td>del (0.0204)</td>\n",
       "      <td>al (0.0195)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Había una vez más, el hombre que</td>\n",
       "      <td>había (0.1757)</td>\n",
       "      <td>se (0.0684)</td>\n",
       "      <td>estaba (0.0556)</td>\n",
       "      <td>le (0.0475)</td>\n",
       "      <td>la (0.0471)</td>\n",
       "      <td>tenía (0.0348)</td>\n",
       "      <td>lo (0.0310)</td>\n",
       "      <td>me (0.0221)</td>\n",
       "      <td>no (0.0146)</td>\n",
       "      <td>amaba (0.0139)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Había una vez más, el hombre que había</td>\n",
       "      <td>sido (0.0862)</td>\n",
       "      <td>estado (0.0856)</td>\n",
       "      <td>visto (0.0478)</td>\n",
       "      <td>hablado (0.0323)</td>\n",
       "      <td>conocido (0.0255)</td>\n",
       "      <td>intentado (0.0232)</td>\n",
       "      <td>hecho (0.0230)</td>\n",
       "      <td>matado (0.0214)</td>\n",
       "      <td>en (0.0184)</td>\n",
       "      <td>entrado (0.0129)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Había una vez más, el hombre que había sido</td>\n",
       "      <td>su (0.1692)</td>\n",
       "      <td>, (0.0989)</td>\n",
       "      <td>el (0.0616)</td>\n",
       "      <td>. (0.0441)</td>\n",
       "      <td>antes (0.0383)</td>\n",
       "      <td>en (0.0360)</td>\n",
       "      <td>y (0.0337)</td>\n",
       "      <td>capaz (0.0231)</td>\n",
       "      <td>durante (0.0221)</td>\n",
       "      <td>siempre (0.0183)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Había una vez más, el hombre que había sido su</td>\n",
       "      <td>padre (0.1336)</td>\n",
       "      <td>compañero (0.1296)</td>\n",
       "      <td>amigo (0.1290)</td>\n",
       "      <td>hermano (0.0368)</td>\n",
       "      <td>marido (0.0357)</td>\n",
       "      <td>primer (0.0353)</td>\n",
       "      <td>hijo (0.0245)</td>\n",
       "      <td>jefe (0.0181)</td>\n",
       "      <td>mejor (0.0178)</td>\n",
       "      <td>amante (0.0173)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Había una vez más, el hombre que había sido su...</td>\n",
       "      <td>, (0.1805)</td>\n",
       "      <td>había (0.1334)</td>\n",
       "      <td>. (0.1022)</td>\n",
       "      <td>y (0.0761)</td>\n",
       "      <td>se (0.0562)</td>\n",
       "      <td>no (0.0505)</td>\n",
       "      <td>en (0.0381)</td>\n",
       "      <td>le (0.0301)</td>\n",
       "      <td>durante (0.0280)</td>\n",
       "      <td>era (0.0277)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Había una vez más, el hombre que había sido su...</td>\n",
       "      <td>el (0.1232)</td>\n",
       "      <td>y (0.0614)</td>\n",
       "      <td>que (0.0339)</td>\n",
       "      <td>un (0.0325)</td>\n",
       "      <td>su (0.0319)</td>\n",
       "      <td>había (0.0298)</td>\n",
       "      <td>se (0.0236)</td>\n",
       "      <td>no (0.0193)</td>\n",
       "      <td>en (0.0161)</td>\n",
       "      <td>era (0.0159)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Había una vez más, el hombre que había sido su...</td>\n",
       "      <td>que (0.2648)</td>\n",
       "      <td>hombre (0.1358)</td>\n",
       "      <td>padre (0.0359)</td>\n",
       "      <td>señor (0.0272)</td>\n",
       "      <td>hijo (0.0235)</td>\n",
       "      <td>único (0.0214)</td>\n",
       "      <td>de (0.0165)</td>\n",
       "      <td>mismo (0.0148)</td>\n",
       "      <td>más (0.0115)</td>\n",
       "      <td>hermano (0.0096)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Había una vez más, el hombre que había sido su...</td>\n",
       "      <td>había (0.4517)</td>\n",
       "      <td>le (0.0951)</td>\n",
       "      <td>se (0.0635)</td>\n",
       "      <td>la (0.0389)</td>\n",
       "      <td>lo (0.0228)</td>\n",
       "      <td>no (0.0184)</td>\n",
       "      <td>estaba (0.0114)</td>\n",
       "      <td>tenía (0.0102)</td>\n",
       "      <td>siempre (0.0099)</td>\n",
       "      <td>, (0.0094)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Había una vez más, el hombre que había sido su...</td>\n",
       "      <td>sido (0.1529)</td>\n",
       "      <td>estado (0.0628)</td>\n",
       "      <td>tenido (0.0455)</td>\n",
       "      <td>hecho (0.0314)</td>\n",
       "      <td>intentado (0.0235)</td>\n",
       "      <td>llevado (0.0172)</td>\n",
       "      <td>sabido (0.0169)</td>\n",
       "      <td>vivido (0.0136)</td>\n",
       "      <td>muerto (0.0135)</td>\n",
       "      <td>dado (0.0118)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Había una vez más, el hombre que había sido su...</td>\n",
       "      <td>su (0.5971)</td>\n",
       "      <td>el (0.1628)</td>\n",
       "      <td>un (0.0241)</td>\n",
       "      <td>, (0.0165)</td>\n",
       "      <td>la (0.0141)</td>\n",
       "      <td>mi (0.0110)</td>\n",
       "      <td>siempre (0.0104)</td>\n",
       "      <td>tan (0.0093)</td>\n",
       "      <td>y (0.0068)</td>\n",
       "      <td>capaz (0.0058)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input         Choice #1  \\\n",
       "0                                       Había una vez      más (0.4087)   \n",
       "1                                   Había una vez más        , (0.5323)   \n",
       "2                                  Había una vez más,       el (0.0491)   \n",
       "3                               Había una vez más, el   hombre (0.0230)   \n",
       "4                        Había una vez más, el hombre      que (0.1364)   \n",
       "5                    Había una vez más, el hombre que    había (0.1757)   \n",
       "6              Había una vez más, el hombre que había     sido (0.0862)   \n",
       "7         Había una vez más, el hombre que había sido       su (0.1692)   \n",
       "8      Había una vez más, el hombre que había sido su    padre (0.1336)   \n",
       "9   Había una vez más, el hombre que había sido su...        , (0.1805)   \n",
       "10  Había una vez más, el hombre que había sido su...       el (0.1232)   \n",
       "11  Había una vez más, el hombre que había sido su...      que (0.2648)   \n",
       "12  Había una vez más, el hombre que había sido su...    había (0.4517)   \n",
       "13  Había una vez más, el hombre que había sido su...     sido (0.1529)   \n",
       "14  Había una vez más, el hombre que había sido su...       su (0.5971)   \n",
       "\n",
       "              Choice #2         Choice #3          Choice #4  \\\n",
       "0          que (0.1055)       en (0.0727)         , (0.0490)   \n",
       "1           en (0.0701)        . (0.0519)        la (0.0194)   \n",
       "2           se (0.0456)       la (0.0409)        en (0.0339)   \n",
       "3        hecho (0.0225)    señor (0.0173)     joven (0.0165)   \n",
       "4           se (0.0979)       de (0.0590)     había (0.0475)   \n",
       "5           se (0.0684)   estaba (0.0556)        le (0.0475)   \n",
       "6       estado (0.0856)    visto (0.0478)   hablado (0.0323)   \n",
       "7            , (0.0989)       el (0.0616)         . (0.0441)   \n",
       "8    compañero (0.1296)    amigo (0.1290)   hermano (0.0368)   \n",
       "9        había (0.1334)        . (0.1022)         y (0.0761)   \n",
       "10           y (0.0614)      que (0.0339)        un (0.0325)   \n",
       "11      hombre (0.1358)    padre (0.0359)     señor (0.0272)   \n",
       "12          le (0.0951)       se (0.0635)        la (0.0389)   \n",
       "13      estado (0.0628)   tenido (0.0455)     hecho (0.0314)   \n",
       "14          el (0.1628)       un (0.0241)         , (0.0165)   \n",
       "\n",
       "              Choice #5            Choice #6          Choice #7  \\\n",
       "0         allí (0.0099)          se (0.0087)         . (0.0082)   \n",
       "1          que (0.0191)           : (0.0174)        el (0.0162)   \n",
       "2            y (0.0301)          no (0.0299)     había (0.0205)   \n",
       "3     recuerdo (0.0157)    muchacho (0.0120)     mundo (0.0119)   \n",
       "4           no (0.0358)          le (0.0276)    estaba (0.0216)   \n",
       "5           la (0.0471)       tenía (0.0348)        lo (0.0310)   \n",
       "6     conocido (0.0255)   intentado (0.0232)     hecho (0.0230)   \n",
       "7        antes (0.0383)          en (0.0360)         y (0.0337)   \n",
       "8       marido (0.0357)      primer (0.0353)      hijo (0.0245)   \n",
       "9           se (0.0562)          no (0.0505)        en (0.0381)   \n",
       "10          su (0.0319)       había (0.0298)        se (0.0236)   \n",
       "11        hijo (0.0235)       único (0.0214)        de (0.0165)   \n",
       "12          lo (0.0228)          no (0.0184)    estaba (0.0114)   \n",
       "13   intentado (0.0235)     llevado (0.0172)    sabido (0.0169)   \n",
       "14          la (0.0141)          mi (0.0110)   siempre (0.0104)   \n",
       "\n",
       "            Choice #8          Choice #9         Choice #10  \n",
       "0     dentro (0.0081)         a (0.0078)        al (0.0078)  \n",
       "1         se (0.0157)         a (0.0142)         y (0.0113)  \n",
       "2         me (0.0194)        su (0.0158)       los (0.0157)  \n",
       "3    corazón (0.0105)    cuerpo (0.0095)     deseo (0.0087)  \n",
       "4         la (0.0205)       del (0.0204)        al (0.0195)  \n",
       "5         me (0.0221)        no (0.0146)     amaba (0.0139)  \n",
       "6     matado (0.0214)        en (0.0184)   entrado (0.0129)  \n",
       "7      capaz (0.0231)   durante (0.0221)   siempre (0.0183)  \n",
       "8       jefe (0.0181)     mejor (0.0178)    amante (0.0173)  \n",
       "9         le (0.0301)   durante (0.0280)       era (0.0277)  \n",
       "10        no (0.0193)        en (0.0161)       era (0.0159)  \n",
       "11     mismo (0.0148)       más (0.0115)   hermano (0.0096)  \n",
       "12     tenía (0.0102)   siempre (0.0099)         , (0.0094)  \n",
       "13    vivido (0.0136)    muerto (0.0135)      dado (0.0118)  \n",
       "14       tan (0.0093)         y (0.0068)     capaz (0.0058)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legend_prompt = \"Cuenta la leyenda que en el valle escondido de Brumalia, \"\n",
    "output_text, iterations_df = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    legend_prompt,\n",
    "    max_length=25,\n",
    "    eps=1.0,\n",
    "    top_n=10,\n",
    "    return_iterations=True,\n",
    "    device=device,\n",
    ")\n",
    "print(output_text)\n",
    "iterations_df.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce17648d",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2 Mezcla (ε = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab4a70c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Había una vez fuera, probablemente Ollie y ella se convirtieron en estudiantiles de universidad. \n",
      "\n",
      "—No sé dónde vivir —dijo Ollie—. Hay dos clases. Ollie es una estudiante de segundo nivel, y no necesita que la llamen para que opere. \n",
      "\n",
      "—Lo discutiré contigo —dijo Ollie—. Y te haré una visita. \n",
      "\n",
      "—No te preocupes por eso —dijo Ollie—. No es buena idea. \n",
      "\n",
      "—Yo también, Florian. \n",
      "\n",
      "—No te\n"
     ]
    }
   ],
   "source": [
    "output_text, _ = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    legend_prompt,\n",
    "    max_length=120,\n",
    "    eps=0.5,\n",
    "    device=device,\n",
    ")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13c6552",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Generación con `model.generate` (Transformers)\n",
    "Más parámetros: `temperature`, `top_k`, `top_p`, `repetition_penalty`, etc.\n",
    "Documentación: \n",
    "- https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig\n",
    "- https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationMixin.generate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb8d4a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Había una vez más sin saber qué hacer para detener el impulso de la furia, y en un momento dado se dio cuenta de que si no lo hacía ya lo sabría. \n",
      "\n",
      "—Le enseñaré a la policía —dijo—. Yo estoy en la sala de interrogatorios y no he venido aquí para que me encierren. \n",
      "\n",
      "—Le enseñaré a la policía —dijo el inspector, señalando la puerta—. Y ahora, si no me van a hacer algo, le diré que me han encerrado aquí. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(legend_prompt, return_tensors='pt').to(device)\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    ")\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d116a2",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Ajuste fino (fine‑tuning) con un corpus propio\n",
    "\n",
    "Para ilustrar el flujo, ofrecemos **dos opciones**:\n",
    "1. **Corpus local**: un archivo `JSONL` con una clave `text` por fila.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf1e054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:00, ? examples/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating train split: 10 examples [00:00, 192.57 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "data_path = \"/Users/j.ricardomunoz/Desktop/Practica 5/leyendas_ficticias.jsonl\"\n",
    "dataset = load_dataset('json', data_files={'train': data_path})\n",
    "\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84faddcd",
   "metadata": {},
   "source": [
    "\n",
    "### 6.1 EDA rápido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f088ff68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'En el pueblo de Brumalia, cada solsticio se abría una puerta luminosa en la plaza. Los ancianos decían que conducía al taller de los vientos, donde artesanos invisibles tejían corrientes para el nuevo año.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset['train'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff5f82ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Palabras por registro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>En el pueblo de Brumalia, cada solsticio se ab...</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cuando la montaña Mirabal despertó, no fue con...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La biblioteca sumergida de Salmorán se dejaba ...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Un puñado de niños descubrió que las luciérnag...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cuenta la leyenda que el reloj del faro de Ven...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  Palabras por registro\n",
       "0  En el pueblo de Brumalia, cada solsticio se ab...                     34\n",
       "1  Cuando la montaña Mirabal despertó, no fue con...                     32\n",
       "2  La biblioteca sumergida de Salmorán se dejaba ...                     31\n",
       "3  Un puñado de niños descubrió que las luciérnag...                     30\n",
       "4  Cuenta la leyenda que el reloj del faro de Ven...                     29"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset.set_format('pandas')\n",
    "df = dataset['train'].to_pandas()\n",
    "df['Palabras por registro'] = df['text'].astype(str).str.split().apply(len)\n",
    "df[['text', 'Palabras por registro']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4253521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10.00000\n",
       "mean     31.00000\n",
       "std       2.44949\n",
       "min      27.00000\n",
       "25%      29.25000\n",
       "50%      30.50000\n",
       "75%      33.50000\n",
       "max      34.00000\n",
       "Name: Palabras por registro, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df['Palabras por registro'].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79f1686",
   "metadata": {},
   "source": [
    "\n",
    "### 6.2 Tokenización y *train/test split*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5a88552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 383.14 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 8\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset.reset_format()\n",
    "\n",
    "def preprocess_function(max_len):\n",
    "    def _preprocess(examples):\n",
    "        return tokenizer(examples['text'], max_length=max_len, truncation=True, padding='max_length')\n",
    "    return _preprocess\n",
    "\n",
    "tokenized = dataset['train'].map(preprocess_function(max_len=128), batched=True)\n",
    "tokenized = tokenized.remove_columns([c for c in tokenized.column_names if c != 'input_ids'])\n",
    "tokenized = tokenized.train_test_split(train_size=0.8, seed=42)\n",
    "tokenized.set_format('torch')\n",
    "tokenized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d38f5a",
   "metadata": {},
   "source": [
    "\n",
    "### 6.3 Entrenamiento\n",
    "Reducimos `batch_size` y épocas por ser un ejemplo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d99abbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 01:06, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.616600</td>\n",
       "      <td>4.763004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.817100</td>\n",
       "      <td>4.653461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.073900</td>\n",
       "      <td>4.594625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.592300</td>\n",
       "      <td>4.571231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.387900</td>\n",
       "      <td>4.569223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=3.2975602626800535, metrics={'train_runtime': 68.5571, 'train_samples_per_second': 0.583, 'train_steps_per_second': 0.292, 'total_flos': 2612920320000.0, 'train_loss': 3.2975602626800535, 'epoch': 5.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 2\n",
    "logging_steps = max(1, len(tokenized['train']) // batch_size)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./hf-gpt-finetuned',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    train_dataset=tokenized['train'],\n",
    "    eval_dataset=tokenized['test'],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba337a6",
   "metadata": {},
   "source": [
    "\n",
    "### 6.4 Generación después del ajuste fino\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c803864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Había una vez que un hombre se acercaba a ella, su corazón latía a mil por hora. \n",
      "\n",
      "—Tengo asuntos más urgentes que atender, si quiere tomar represalias. \n",
      "\n",
      "—Daré órdenes a los hombres para que se unan a nosotros. \n",
      "\n",
      "—No voy a permitir que los demás hagan ningún movimiento hacia mí. \n",
      "\n",
      "—El que no quiera que me dejen marchar al campo es un amigo de la familia. Puedo encontrar a un hombre que quiera que me lleve conmigo, pero no puedo hacer nada para impedirlo. \n",
      "\n",
      "—Por favor, diles que no se retire\n"
     ]
    }
   ],
   "source": [
    "# Si has ejecutado el entrenamiento arriba, el modelo ya está actualizado.\n",
    "inputs = tokenizer(legend_prompt, return_tensors='pt').to(device)\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_length=120,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "    )\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02cf663b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Había una vez construidas, ovieres y telescopios trabajaban en silencio. No vivían en el espacio abierto. Cada uno de los dos Estados constituía una parte del sistema. \n",
      "\n",
      "—Muy bien —dijo el capitán—. ¿Qué hay del sistema? \n",
      "\n",
      "—El del Laboratorio Médico Clínica ocurrió hace unos años. Más tarde, en el año 2002, examinaron los datos de la vida de astronautas en elquina. La información descubrió que el patrón de vida de astronautas era el mismo que creemos. \n",
      "\n",
      "—¿Y, entonces? \n",
      "\n",
      "—No lo sabemos. \n",
      "\n",
      "Gardner no dijo nada,\n"
     ]
    }
   ],
   "source": [
    "legend_prompts = [\n",
    "    \"Cuenta la leyenda que en el valle escondido de Brumalia, \",\n",
    "    \"Dicen los ancianos de Mirabal que \",\n",
    "    \"En las noches de luna roja de Salmorán, \",\n",
    "]\n",
    "\n",
    "samples = []\n",
    "for prompt in legend_prompts:\n",
    "    generated, _ = generate(model, tokenizer, prompt, max_length=120, eps=0.3, device=device)\n",
    "    samples.append({\"prompt\": prompt, \"leyenda_generada\": generated})\n",
    "\n",
    "pd.DataFrame(samples)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
